# LLaMA 2 - Every Resource you need

> ## Excerpt
> All Resources for LLaMA 2, How to test, train, and deploy it.

---
LLaMA 2 is a large language model developed by Meta and is the successor to LLaMA 1. LLaMA 2 is available for free for research and commercial use through providers like AWS, Hugging Face, and others. LLaMA 2 pretrained models are trained on 2 trillion tokens, and have double the context length than LLaMA 1. Its fine-tuned models have been trained on over 1 million human annotations.

This blog post includes all relevant resources to help get started quickly. It includes links to:

-   [What is LLaMA 2?](https://www.philschmid.de/llama-2#what-is-llama-2)
-   [Playgrounds, where you can test the model](https://www.philschmid.de/llama-2#llama-playgrounds-test-it)
-   [The research behind the model](https://www.philschmid.de/llama-2#research-behind-llama-2)
-   [How good the model is, benchmarks](https://www.philschmid.de/llama-2#how-good-is-llama-2-benchmarks)
-   [How to correctly prompt the chat model](https://www.philschmid.de/llama-2#how-to-prompt-llama-2-chat)
-   [How to train the model using PEFT](https://www.philschmid.de/llama-2#how-to-train-llama-2)
-   [How to deploy the model for inference](https://www.philschmid.de/llama-2#how-to-deploy-llama-2)
-   [and other resources](https://www.philschmid.de/llama-2#other-sources)

The official announcement from Meta can be found here: [https://ai.meta.com/llama/](https://ai.meta.com/llama/)

## What is LLaMa 2?

Meta released LLaMA 2, the new state-of-the-art open large language model (LLM). LLaMA 2 represents the next iteration of LLaMA and comes with a commercially-permissive license. LLaMA 2 comes in 3 different sizes - 7B, 13B, and 70B parameters. New improvements compared to the original LLaMA include:

-   Trained on 2 trillion tokens of text data
-   Allows commercial use
-   Uses a 4096 default context window ([can be expanded](https://twitter.com/joao_gante/status/1681593605541236736?s=20))
-   The 70B model adopts grouped-query attention (GQA)
-   Available on [Hugging Face Hub](https://huggingface.co/models?other=llama-2)

## LLaMA Playgrounds, test it

There are a few different playgrounds available to test out interacting with LLaMA 2 Chat:

-   [HuggingChat](https://huggingface.co/chat) allows you to chat with the LLaMA 2 70B model through Hugging Face's conversational interface. This provides a simple way to see the chatbot in action.
-   [Hugging Face Spaces](https://huggingface.co/spaces) has LLaMA 2 models in [7B](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat), [13B](https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat) and [70B](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI) sizes available to test. The interactive demos let you compare different model sizes.
-   [Perplexity](https://llama.perplexity.ai/) has both the 7B and 13B LLaMA 2 models accessible through their conversational AI demo. You can chat with the models and provide feedback on the responses.

## Research Behind LLaMA 2

LLaMA 2 is a base LLM model and pretrained on publicly available data found online. Additionally Meta released a CHAT version. The first version of the CHAT model was SFT (Supervised fine-tuned) model. After that, LLaMA-2-chat was iteratively improved through Reinforcement Learning from Human Feedback (RLHF). The RLHF process involved techniques like rejection sampling and proximal policy optimization (PPO) to further refine the chatbot. Meta only released the latest RLHF (v5) versions of the model. If you curious how the process was behind checkout:

-   [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
-   [Llama 2: an incredible open LLM](https://www.interconnects.ai/p/llama-2-from-meta)
-   [Llama 2: Full Breakdown](https://www.youtube.com/watch?v=zJBpRn2zTco&ab_channel=AIExplained)

## How good is LLaMA 2, benchmarks?

Meta claims that _“Llama 2 outperforms other open source language models on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests.”._ You can find more insights over the performance at:

-   [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
-   [Meta Announcement](https://ai.meta.com/llama/)

## How to Prompt LLaMA 2 Chat

LLaMA 2 Chat is an open conversational model. Interacting with LLaMA 2 Chat effectively requires providing the right prompts and questions to produce coherent and useful responses. Meta didn’t choose the simplest prompt. Below is the prompt template for single-turn and multi-turn conversations. This template follows the model's training procedure, as described in [the LLaMA 2 paper](https://huggingface.co/papers/2307.09288). You can also take a look at [LLaMA 2 Prompt Template](https://gpus.llm-utils.org/llama-2-prompt-template/).

Single-turn

```
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_message }} [/INST]
```

Multi-turn

```
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST] {{ model_answer_2 }} </s><s>[INST] {{ user_msg_3 }} [/INST]
```

## How to train LLaMA 2

LLaMA 2 is openly available making it easy to fine-tune using techniques, .e.g. PEFT. There are great resources available for training your own versions of LLaMA 2:

-   [Extended Guide: Instruction-tune Llama 2](https://www.philschmid.de/instruction-tune-llama-2)
-   [Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora)
-   [Fine-tuning with PEFT](https://huggingface.co/blog/llama2#fine-tuning-with-peft)
-   [Meta Examples and recipes for Llama model](https://github.com/facebookresearch/llama-recipes/tree/main)
-   [The EASIEST way to finetune LLAMA-v2 on local machine!](https://www.youtube.com/watch?v=3fsn19OI_C8&ab_channel=AbhishekThakur)

## How to Deploy LLaMA 2

LLaMA 2 can be deployed in local environment ([llama.cpp](https://github.com/ggerganov/llama.cpp)), using managed services like [Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) or through or cloud platforms like AWS, Google Cloud, and Microsoft Azure.

-   [Deploy LLaMa 2 Using text-generation-inference and Inference Endpoints](https://huggingface.co/blog/llama2#using-text-generation-inference-and-inference-endpoints)
-   Deploy LLaMA 2 70B using Amazon SageMaker (coming soon)
-   [Llama-2-13B-chat locally on your M1/M2 Mac with GPU inference](https://gist.github.com/adrienbrault/b76631c56c736def9bc1bc2167b5d129)

## Other Sources

-   [Llama 2 Resources](https://gpus.llm-utils.org/llama-2-resources/)

Let me know if you would like me to expand on any section or add additional details. I aimed to provide a high-level overview of key information related to LLaMA 2's release based on what is publicly known so far.